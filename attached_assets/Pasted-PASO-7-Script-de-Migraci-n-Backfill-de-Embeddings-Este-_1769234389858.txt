PASO 7: Script de Migraci√≥n (Backfill de Embeddings)
Este script genera embeddings para todos los chunks existentes que no los tienen.
Cre√° este archivo:
backend/scripts/migrate_embeddings.py
python"""
Script de migraci√≥n para generar embeddings de chunks existentes.
Ejecutar una sola vez despu√©s de implementar pgvector.

Uso:
    python -m scripts.migrate_embeddings
    
    # O con l√≠mite para probar:
    python -m scripts.migrate_embeddings --limit 100
    
    # Solo una empresa espec√≠fica:
    python -m scripts.migrate_embeddings --empresa-id abc123
"""

import asyncio
import asyncpg
import os
import sys
import argparse
from datetime import datetime
from typing import Optional

# Agregar el directorio padre al path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.embedding_service import EmbeddingService


# Configuraci√≥n
DATABASE_URL = os.getenv("DATABASE_URL")
BATCH_SIZE = 50  # Procesar de a 50 chunks


class MigrationStats:
    def __init__(self):
        self.total = 0
        self.processed = 0
        self.errors = 0
        self.skipped = 0
        self.start_time = None
    
    def start(self):
        self.start_time = datetime.now()
    
    def elapsed(self) -> str:
        if not self.start_time:
            return "0s"
        delta = datetime.now() - self.start_time
        return f"{delta.seconds}s"
    
    def rate(self) -> float:
        if not self.start_time:
            return 0
        delta = (datetime.now() - self.start_time).seconds or 1
        return self.processed / delta
    
    def print_progress(self):
        pct = (self.processed / self.total * 100) if self.total > 0 else 0
        print(
            f"\r‚è≥ Progreso: {self.processed}/{self.total} ({pct:.1f}%) | "
            f"Errores: {self.errors} | "
            f"Velocidad: {self.rate():.1f} chunks/s | "
            f"Tiempo: {self.elapsed()}",
            end="", flush=True
        )


async def get_chunks_without_embeddings(
    db: asyncpg.Connection,
    empresa_id: Optional[str] = None,
    limit: Optional[int] = None
) -> list:
    """Obtiene chunks que no tienen embedding."""
    
    sql = """
        SELECT id, contenido, empresa_id
        FROM knowledge_chunks
        WHERE embedding IS NULL
        AND contenido IS NOT NULL
        AND LENGTH(contenido) > 10
    """
    
    params = []
    param_count = 0
    
    if empresa_id:
        param_count += 1
        sql += f" AND empresa_id = ${param_count}"
        params.append(empresa_id)
    
    sql += " ORDER BY id"
    
    if limit:
        param_count += 1
        sql += f" LIMIT ${param_count}"
        params.append(limit)
    
    return await db.fetch(sql, *params)


async def update_chunk_embedding(
    db: asyncpg.Connection,
    chunk_id: str,
    embedding: list
) -> bool:
    """Actualiza el embedding de un chunk."""
    
    try:
        # Convertir lista a formato pgvector
        embedding_str = "[" + ",".join(map(str, embedding)) + "]"
        
        await db.execute("""
            UPDATE knowledge_chunks
            SET embedding = $1::vector
            WHERE id = $2
        """, embedding_str, chunk_id)
        
        return True
    except Exception as e:
        print(f"\n‚ùå Error actualizando chunk {chunk_id}: {e}")
        return False


async def migrate_batch(
    db: asyncpg.Connection,
    embedder: EmbeddingService,
    chunks: list,
    stats: MigrationStats
):
    """Procesa un batch de chunks."""
    
    # Extraer textos
    texts = [chunk["contenido"] for chunk in chunks]
    chunk_ids = [chunk["id"] for chunk in chunks]
    
    try:
        # Generar embeddings en batch
        results = await embedder.generate_batch_embeddings(texts)
        
        # Actualizar cada chunk
        for chunk_id, result in zip(chunk_ids, results):
            success = await update_chunk_embedding(db, chunk_id, result.embedding)
            
            if success:
                stats.processed += 1
            else:
                stats.errors += 1
            
            stats.print_progress()
    
    except Exception as e:
        print(f"\n‚ùå Error en batch: {e}")
        stats.errors += len(chunks)


async def run_migration(
    empresa_id: Optional[str] = None,
    limit: Optional[int] = None,
    dry_run: bool = False
):
    """Ejecuta la migraci√≥n completa."""
    
    print("=" * 60)
    print("üöÄ MIGRACI√ìN DE EMBEDDINGS PARA KNOWLEDGE CHUNKS")
    print("=" * 60)
    
    if not DATABASE_URL:
        print("‚ùå ERROR: DATABASE_URL no configurada")
        return
    
    # Conectar a DB
    print("\nüì° Conectando a la base de datos...")
    db = await asyncpg.connect(DATABASE_URL)
    
    # Inicializar embedder
    print("ü§ñ Inicializando servicio de embeddings...")
    embedder = EmbeddingService()
    print(f"   Provider: {embedder.provider}")
    print(f"   Model: {embedder.model}")
    
    # Obtener chunks pendientes
    print("\nüìä Obteniendo chunks sin embeddings...")
    chunks = await get_chunks_without_embeddings(db, empresa_id, limit)
    
    if not chunks:
        print("‚úÖ No hay chunks pendientes de procesar!")
        await db.close()
        return
    
    # Stats
    stats = MigrationStats()
    stats.total = len(chunks)
    
    print(f"   Total chunks a procesar: {stats.total}")
    
    if empresa_id:
        print(f"   Filtrado por empresa: {empresa_id}")
    
    # Dry run
    if dry_run:
        print("\nüîç DRY RUN - No se guardar√°n cambios")
        print(f"   Se procesar√≠an {stats.total} chunks")
        
        # Mostrar muestra
        print("\n   Muestra de chunks:")
        for chunk in chunks[:3]:
            preview = chunk["contenido"][:100].replace("\n", " ")
            print(f"   - [{chunk['id'][:8]}...] {preview}...")
        
        await db.close()
        return
    
    # Confirmaci√≥n
    print(f"\n‚ö†Ô∏è  Se van a procesar {stats.total} chunks")
    print(f"   Costo estimado: ~${stats.total * 0.0001:.2f} USD (OpenAI ada-002)")
    
    confirm = input("\n¬øContinuar? [y/N]: ")
    if confirm.lower() != 'y':
        print("‚ùå Migraci√≥n cancelada")
        await db.close()
        return
    
    # Ejecutar migraci√≥n
    print("\nüîÑ Iniciando migraci√≥n...\n")
    stats.start()
    
    for i in range(0, len(chunks), BATCH_SIZE):
        batch = chunks[i:i + BATCH_SIZE]
        await migrate_batch(db, embedder, batch, stats)
        
        # Peque√±a pausa entre batches para no saturar la API
        if i + BATCH_SIZE < len(chunks):
            await asyncio.sleep(0.5)
    
    # Resumen final
    print("\n\n" + "=" * 60)
    print("üìà RESUMEN DE MIGRACI√ìN")
    print("=" * 60)
    print(f"   ‚úÖ Procesados exitosamente: {stats.processed}")
    print(f"   ‚ùå Errores: {stats.errors}")
    print(f"   ‚è±Ô∏è  Tiempo total: {stats.elapsed()}")
    print(f"   üìä Velocidad promedio: {stats.rate():.1f} chunks/s")
    
    # Verificar resultado
    remaining = await db.fetchval("""
        SELECT COUNT(*) FROM knowledge_chunks
        WHERE embedding IS NULL
        AND contenido IS NOT NULL
        AND LENGTH(contenido) > 10
    """)
    
    print(f"\n   üìã Chunks a√∫n sin embedding: {remaining}")
    
    # Cerrar conexiones
    await embedder.close()
    await db.close()
    
    print("\n‚úÖ Migraci√≥n completada!")


def main():
    parser = argparse.ArgumentParser(description="Migrar chunks a embeddings")
    parser.add_argument("--empresa-id", help="Filtrar por empresa espec√≠fica")
    parser.add_argument("--limit", type=int, help="L√≠mite de chunks a procesar")
    parser.add_argument("--dry-run", action="store_true", help="Solo mostrar qu√© se har√≠a")
    
    args = parser.parse_args()
    
    asyncio.run(run_migration(
        empresa_id=args.empresa_id,
        limit=args.limit,
        dry_run=args.dry_run
    ))


if __name__ == "__main__":
    main()

PASO 8: Ejecutar la migraci√≥n
Primero prob√° en dry-run:
bashpython -m scripts.migrate_embeddings --dry-run
Luego con un l√≠mite peque√±o para verificar:
bashpython -m scripts.migrate_embeddings --limit 50
Finalmente la migraci√≥n completa:
bashpython -m scripts.migrate_embeddings

PASO 9: Integrar en el pipeline de ingesta
Ahora hay que modificar tu ingestion_service.py para que los nuevos documentos generen embeddings autom√°ticamente.
Busc√° donde guard√°s los chunks y agreg√° esto:
En backend/services/ingestion_service.py (o donde proceses documentos):
pythonfrom services.embedding_service import get_embedding_service

class IngestionService:
    def __init__(self, db):
        self.db = db
        self.embedder = get_embedding_service()
    
    async def process_document(self, file, empresa_id: str, ...):
        # ... tu c√≥digo existente de extracci√≥n y chunking ...
        
        # Despu√©s de crear los chunks:
        chunks = self.chunking_service.chunk_document(text)
        
        # NUEVO: Generar embeddings para los chunks
        print(f"Generando embeddings para {len(chunks)} chunks...")
        
        texts = [chunk["contenido"] for chunk in chunks]
        embedding_results = await self.embedder.generate_batch_embeddings(texts)
        
        # Guardar chunks CON embeddings
        for chunk, emb_result in zip(chunks, embedding_results):
            embedding_str = "[" + ",".join(map(str, emb_result.embedding)) + "]"
            
            await self.db.execute("""
                INSERT INTO knowledge_chunks 
                (document_id, empresa_id, chunk_index, contenido, tokens_count, embedding)
                VALUES ($1, $2, $3, $4, $5, $6::vector)
            """, 
                doc_id, 
                empresa_id, 
                chunk["index"], 
                chunk["contenido"],
                chunk.get("tokens", 0),
                embedding_str
            )
        
        print(f"‚úÖ Documento procesado con {len(chunks)} chunks + embeddings")

PASO 10: Actualizar agent_service para usar RAG sem√°ntico
Busc√° en tu agent_service.py donde constru√≠s el contexto para los agentes y reemplaz√° la b√∫squeda vieja:
pythonfrom services.vector_search_service import VectorSearchService

class AgentService:
    def __init__(self, db):
        self.db = db
        self.vector_search = VectorSearchService(db)
    
    async def _get_rag_context(self, empresa_id: str, query: str, agent_id: str) -> str:
        """Obtiene contexto relevante del KB para el agente."""
        
        # NUEVO: Usar b√∫squeda sem√°ntica h√≠brida
        context = await self.vector_search.get_context_for_agent(
            empresa_id=empresa_id,
            query=query,
            agent_id=agent_id,
            max_tokens=4000
        )
        
        return context
    
    async def chat(self, empresa_id: str, agent_id: str, message: str, ...):
        # Obtener contexto RAG
        rag_context = await self._get_rag_context(empresa_id, message, agent_id)
        
        # Construir prompt con contexto
        system_prompt = self._get_system_prompt(agent_id)
        
        if rag_context:
            system_prompt += f"""

## INFORMACI√ìN DEL REPOSITORIO DE CONOCIMIENTO
Usa la siguiente informaci√≥n como referencia para tu an√°lisis:

{rag_context}

---
Basa tu respuesta en esta informaci√≥n cuando sea relevante.
"""
        
        # Llamar a Claude
        response = await self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4096,
            system=system_prompt,
            messages=[{"role": "user", "content": message}]
        )
        
        return response

‚úÖ CHECKLIST DEL RAG SEM√ÅNTICO
#TareaEstado1SQL: Crear extensi√≥n vector‚¨ú2SQL: Agregar columna embedding‚¨ú3SQL: Crear √≠ndices‚¨ú4Crear embedding_service.py‚¨ú5Crear vector_search_service.py‚¨ú6Agregar OPENAI_API_KEY en Secrets‚¨ú7Crear ruta /search/semantic‚¨ú8Crear migrate_embeddings.py‚¨ú9Ejecutar migraci√≥n (dry-run)‚¨ú10Ejecutar migraci√≥n (completa)‚¨ú11Integrar en ingestion_service‚¨ú12Integrar en agent_service‚¨ú

Una vez que completes esto, tu RAG va a funcionar de verdad - buscando por significado, no por palabras exactas.
¬øLo implement√°s y me avis√°s cuando est√© listo para pasar al Tour #2: Refactor Frontend?