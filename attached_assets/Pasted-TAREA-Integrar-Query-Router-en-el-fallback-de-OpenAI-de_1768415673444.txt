TAREA: Integrar Query Router en el fallback de OpenAI del Strategy Agent.

IMPORTANTE: NO toques las llamadas a OpenRouter (multi-LLM council). Solo optimiza el fallback de OpenAI.

PASO 1: En backend/agents/strategy_agent.py, agrega estos imports DESPU√âS de la l√≠nea 28:

```python
from services.query_router import route_query

PASO 2: Modifica la funci√≥n _get_persona_critique() (aproximadamente l√≠nea 558-590):

BUSCA la l√≠nea 581 donde est√°:

response = self.client.chat.completions.create(
    model=self.model,
    messages=[...],
    temperature=0.7
)

REEMPL√ÅZALA con:

# Query Router para fallback OpenAI
enable_router = os.getenv("ENABLE_QUERY_ROUTER", "true").lower() == "true"

if enable_router:
    routing = route_query(
        prompt=messages[-1]["content"] if messages else "",
        task_type="validation"  # Strategy agent valida, usa tier "validation"
    )
    selected_model = routing["model"]
    logger.info(f"üéØ Strategy Router (Persona): {routing['model']} | Tokens: {routing['token_count']} | Cost: ${routing['estimated_cost']:.6f} | {routing['reasoning']}")
else:
    selected_model = self.model

response = self.client.chat.completions.create(
    model=selected_model,  # ‚Üê Din√°mico
    messages=messages,
    temperature=0.7
)

PASO 3: Modifica la funci√≥n _chairman_phase() (aproximadamente l√≠nea 603-650):

BUSCA la l√≠nea 641 donde est√°:

response = self.client.chat.completions.create(
    model=self.model,
    messages=[...],
    temperature=0.3
)

REEMPL√ÅZALA con:

# Query Router para Chairman consolidation
enable_router = os.getenv("ENABLE_QUERY_ROUTER", "true").lower() == "true"

if enable_router:
    routing = route_query(
        prompt=messages[-1]["content"] if messages else "",
        task_type="reasoning"  # Chairman consolida, usa "reasoning"
    )
    selected_model = routing["model"]
    logger.info(f"üéØ Strategy Router (Chairman): {routing['model']} | Tokens: {routing['token_count']} | Cost: ${routing['estimated_cost']:.6f}")
else:
    selected_model = self.model

response = self.client.chat.completions.create(
    model=selected_model,  # ‚Üê Din√°mico
    messages=messages,
    temperature=0.3
)

RESUMEN DE CAMBIOS:

‚úÖ Import de route_query agregado
‚úÖ 2 llamadas OpenAI optimizadas (_get_persona_critique, _chairman_phase)
‚ùå OpenRouter multi-LLM council NO modificado (esos modelos ya est√°n definidos estrat√©gicamente)
VERIFICA que no hay errores de sintaxis y reinicia el backend.