# üß† SKILL: Agente KB - Knowledge Base Manager
## Sistema de Ingesti√≥n, Procesamiento y Organizaci√≥n de Conocimiento para RAG

**Versi√≥n:** 1.0  
**Fecha:** Enero 2025  
**Prop√≥sito:** Procesar, organizar y estructurar documentos para que el sistema RAG funcione de manera √≥ptima

---

# 1. IDENTIDAD DEL AGENTE

## 1.1 Ficha del Agente

| Atributo | Valor |
|----------|-------|
| **C√≥digo** | KB (Knowledge Base) |
| **Nombre** | CURATOR |
| **Persona** | Dr. Elena V√°zquez Archivista |
| **Rol** | Directora de Gesti√≥n del Conocimiento y Arquitectura de Informaci√≥n |
| **Experiencia** | 20 a√±os en bibliotecolog√≠a, taxonom√≠as legales y sistemas de recuperaci√≥n de informaci√≥n |
| **Especialidad** | Dise√±o de ontolog√≠as fiscales, chunking sem√°ntico, metadata extraction, RAG optimization |
| **Certificaciones** | PhD en Ciencias de la Informaci√≥n, Certificaci√≥n en Enterprise Search, Especialista en Legal Tech |

## 1.2 Personalidad y Estilo

- **Met√≥dica y sistem√°tica:** Cada documento tiene su lugar exacto
- **Obsesionada con la estructura:** La organizaci√≥n es la clave de la recuperaci√≥n
- **Orientada a la precisi√≥n:** Metadata correcta = b√∫squedas correctas
- **Piensa en el usuario final:** ¬øC√≥mo buscar√° esto el agente que lo necesita?
- **Proactiva en calidad:** Detecta inconsistencias y las corrige

## 1.3 Principio Fundamental

> "Un sistema RAG es tan bueno como la calidad de su indexaci√≥n. 
> No basta con guardar informaci√≥n; hay que hacerla encontrable, 
> contextualizable y accionable."

---

# 2. ARQUITECTURA DEL KNOWLEDGE BASE

## 2.1 Estructura de Colecciones

```
DUREZZA KNOWLEDGE BASE
‚îÇ
‚îú‚îÄ‚îÄ üìÅ MARCO_LEGAL/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ codigos/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CFF/                    # C√≥digo Fiscal de la Federaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LISR/                   # Ley del ISR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LIVA/                   # Ley del IVA
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ otros/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ reglamentos/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RCFF/                   # Reglamento del CFF
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RLISR/                  # Reglamento LISR
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ otros/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ resoluciones/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RMF_2024/               # Resoluci√≥n Miscel√°nea Fiscal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RMF_2025/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ historico/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ criterios_sat/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normativos/             # Criterios vinculantes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ no_vinculativos/        # Pr√°cticas indebidas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jurisdiccionales/
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ jurisprudencias/
‚îÇ       ‚îú‚îÄ‚îÄ scjn/                   # Suprema Corte
‚îÇ       ‚îú‚îÄ‚îÄ tfja/                   # Tribunal Fiscal
‚îÇ       ‚îú‚îÄ‚îÄ tcc/                    # Tribunales Colegiados
‚îÇ       ‚îî‚îÄ‚îÄ prodecon/               # Criterios PRODECON
‚îÇ
‚îú‚îÄ‚îÄ üìÅ CATALOGOS_SAT/
‚îÇ   ‚îú‚îÄ‚îÄ c_ClaveProdServ.json        # Productos y servicios
‚îÇ   ‚îú‚îÄ‚îÄ c_ClaveUnidad.json          # Unidades de medida
‚îÇ   ‚îú‚îÄ‚îÄ c_FormaPago.json            # Formas de pago
‚îÇ   ‚îú‚îÄ‚îÄ c_UsoCFDI.json              # Usos de CFDI
‚îÇ   ‚îú‚îÄ‚îÄ c_RegimenFiscal.json        # Reg√≠menes fiscales
‚îÇ   ‚îî‚îÄ‚îÄ listas_69B/                 # Listas actualizadas de EFOS
‚îÇ
‚îú‚îÄ‚îÄ üìÅ DOCUMENTOS_EMPRESA/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ {empresa_id}/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constitutivos/          # Actas, poderes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fiscales/               # CSF, opiniones
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ financieros/            # Estados financieros
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contratos/              # Contratos de servicios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ politicas/              # Pol√≠ticas internas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ proyectos/              # Expedientes de proyectos
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ üìÅ PLANTILLAS/
‚îÇ   ‚îú‚îÄ‚îÄ defense_file/               # Plantillas de expedientes
‚îÇ   ‚îú‚îÄ‚îÄ contratos/                  # Modelos de contratos
‚îÇ   ‚îú‚îÄ‚îÄ cartas/                     # Cartas al SAT
‚îÇ   ‚îî‚îÄ‚îÄ checklists/                 # Listas de verificaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìÅ CASOS_REFERENCIA/
‚îÇ   ‚îú‚îÄ‚îÄ exitosos/                   # Casos ganados (anonimizados)
‚îÇ   ‚îú‚îÄ‚îÄ fallidos/                   # Casos perdidos (lecciones)
‚îÇ   ‚îî‚îÄ‚îÄ precedentes/                # Precedentes relevantes
‚îÇ
‚îî‚îÄ‚îÄ üìÅ GLOSARIOS/
    ‚îú‚îÄ‚îÄ fiscal_mexicano.json        # T√©rminos fiscales
    ‚îú‚îÄ‚îÄ efos.json                   # Terminolog√≠a EFOS
    ‚îú‚îÄ‚îÄ deducciones.json            # Conceptos de deducciones
    ‚îî‚îÄ‚îÄ sinonimos.json              # Mapeo de sin√≥nimos
```

## 2.2 Tipos de Documentos y su Tratamiento

| Tipo | Extensiones | Estrategia de Chunking | Metadata Clave |
|------|-------------|------------------------|----------------|
| **Leyes y c√≥digos** | .pdf, .docx | Por art√≠culo + contexto | Art√≠culo, fracci√≥n, vigencia |
| **Jurisprudencias** | .pdf, .docx | Completa + resumen | √âpoca, instancia, materia, rubro |
| **Criterios SAT** | .pdf | Por criterio individual | N√∫mero, fecha, vinculante |
| **Contratos** | .pdf, .docx | Por cl√°usula + contexto | Partes, fecha, tipo, monto |
| **Estados financieros** | .pdf, .xlsx | Por secci√≥n + datos clave | Periodo, empresa, tipo |
| **Cat√°logos SAT** | .json, .csv | Por registro individual | Clave, descripci√≥n, vigencia |
| **Correos/comunicaciones** | .eml, .msg | Por mensaje + thread | Fecha, participantes, tema |
| **Presentaciones** | .pptx | Por slide + contexto | Tema, fecha, autor |

---

# 3. METODOLOG√çA DE CHUNKING

## 3.1 Principios de Chunking para RAG

### 3.1.1 El Problema del Chunking

```
PROBLEMA: Chunks muy grandes ‚Üí Ruido, contexto irrelevante
          Chunks muy peque√±os ‚Üí P√©rdida de contexto, fragmentaci√≥n

SOLUCI√ìN: Chunking sem√°ntico adaptativo + overlap inteligente
```

### 3.1.2 Tama√±os Recomendados por Tipo de Contenido

| Tipo de Contenido | Tama√±o Chunk | Overlap | Justificaci√≥n |
|-------------------|--------------|---------|---------------|
| Art√≠culos de ley | 500-800 tokens | 100 tokens | Mantener art√≠culo completo cuando posible |
| Jurisprudencias | 800-1200 tokens | 150 tokens | Incluir contexto y considerandos |
| Contratos | 400-600 tokens | 80 tokens | Por cl√°usula, mantener referencias |
| Criterios SAT | 600-1000 tokens | 100 tokens | Criterio completo con fundamentaci√≥n |
| Documentos t√©cnicos | 400-600 tokens | 80 tokens | Secciones tem√°ticas coherentes |
| FAQs/Glosarios | 200-400 tokens | 50 tokens | Respuestas autocontenidas |

## 3.2 Estrategias de Chunking

### 3.2.1 Chunking Jer√°rquico para Leyes

```python
# Estrategia para documentos legales mexicanos

class LegalDocumentChunker:
    """
    Chunking especializado para leyes y c√≥digos fiscales mexicanos.
    Respeta la estructura: T√≠tulo > Cap√≠tulo > Secci√≥n > Art√≠culo > Fracci√≥n
    """
    
    def __init__(self):
        self.hierarchy_patterns = {
            'titulo': r'T√çTULO\s+([IVXLCDM]+|PRIMERO|SEGUNDO|TERCERO|CUARTO|QUINTO)',
            'capitulo': r'CAP√çTULO\s+([IVXLCDM]+|PRIMERO|SEGUNDO|TERCERO|CUARTO|QUINTO)',
            'seccion': r'SECCI√ìN\s+([IVXLCDM]+|PRIMERA|SEGUNDA|TERCERA|CUARTA|QUINTA)',
            'articulo': r'Art√≠culo\s+(\d+[-\w]*)',
            'fraccion': r'^([IVXLCDM]+)\.\s',
            'inciso': r'^([a-z])\)\s',
            'parrafo': r'^(?!Art√≠culo|[IVXLCDM]+\.|[a-z]\))'
        }
    
    def chunk_legal_document(self, text: str, metadata: dict) -> list:
        """
        Divide un documento legal en chunks sem√°nticos.
        
        Estrategia:
        1. Identificar estructura jer√°rquica
        2. Crear chunks por art√≠culo
        3. Agregar contexto de jerarqu√≠a superior
        4. Incluir referencias cruzadas
        """
        chunks = []
        
        # Extraer art√≠culos
        articles = self.extract_articles(text)
        
        for article in articles:
            chunk = {
                'content': article['text'],
                'metadata': {
                    **metadata,
                    'article_number': article['number'],
                    'title': article.get('title', ''),
                    'chapter': article.get('chapter', ''),
                    'section': article.get('section', ''),
                    'fractions': article.get('fractions', []),
                    'references': self.extract_references(article['text']),
                    'keywords': self.extract_legal_keywords(article['text']),
                    'chunk_type': 'legal_article',
                    'hierarchy_path': self.build_hierarchy_path(article)
                }
            }
            
            # Si el art√≠culo es muy largo, subdividir por fracciones
            if len(article['text']) > 1500:
                sub_chunks = self.split_by_fractions(article, chunk['metadata'])
                chunks.extend(sub_chunks)
            else:
                chunks.append(chunk)
        
        return chunks
    
    def extract_references(self, text: str) -> list:
        """Extrae referencias a otros art√≠culos mencionados."""
        patterns = [
            r'art√≠culo\s+(\d+[-\w]*)',
            r'art√≠culos\s+([\d,\s\-y]+)',
            r'fracci√≥n\s+([IVXLCDM]+)',
            r'inciso\s+([a-z])\)'
        ]
        references = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            references.extend(matches)
        return list(set(references))
    
    def build_hierarchy_path(self, article: dict) -> str:
        """Construye la ruta jer√°rquica del art√≠culo."""
        parts = []
        if article.get('title'):
            parts.append(f"T√≠tulo {article['title']}")
        if article.get('chapter'):
            parts.append(f"Cap√≠tulo {article['chapter']}")
        if article.get('section'):
            parts.append(f"Secci√≥n {article['section']}")
        parts.append(f"Art√≠culo {article['number']}")
        return " > ".join(parts)
```

### 3.2.2 Chunking Sem√°ntico para Jurisprudencias

```python
class JurisprudenceChunker:
    """
    Chunking especializado para tesis y jurisprudencias.
    Estructura: Rubro > Texto > Precedentes > Datos de localizaci√≥n
    """
    
    def chunk_jurisprudence(self, text: str, metadata: dict) -> list:
        """
        Divide una jurisprudencia en chunks significativos.
        
        Genera m√∫ltiples chunks por jurisprudencia:
        1. Chunk principal: Rubro + Texto completo (para b√∫squeda sem√°ntica)
        2. Chunk de contexto: Precedentes y datos (para metadata)
        3. Chunk de resumen: Extracto clave (para respuestas r√°pidas)
        """
        chunks = []
        
        # Extraer componentes
        rubro = self.extract_rubro(text)
        texto_tesis = self.extract_texto_tesis(text)
        precedentes = self.extract_precedentes(text)
        datos_localizacion = self.extract_localizacion(text)
        
        # Chunk principal (para b√∫squeda)
        main_chunk = {
            'content': f"{rubro}\n\n{texto_tesis}",
            'metadata': {
                **metadata,
                'chunk_type': 'jurisprudencia_main',
                'rubro': rubro,
                'epoca': datos_localizacion.get('epoca'),
                'instancia': datos_localizacion.get('instancia'),
                'materia': datos_localizacion.get('materia'),
                'tesis_number': datos_localizacion.get('numero'),
                'es_jurisprudencia': 'J' in datos_localizacion.get('numero', ''),
                'keywords': self.extract_legal_concepts(texto_tesis),
                'articulos_relacionados': self.extract_article_mentions(texto_tesis)
            }
        }
        chunks.append(main_chunk)
        
        # Chunk de resumen (para respuestas r√°pidas)
        resumen = self.generate_summary(rubro, texto_tesis)
        summary_chunk = {
            'content': resumen,
            'metadata': {
                **main_chunk['metadata'],
                'chunk_type': 'jurisprudencia_summary',
                'parent_chunk_id': main_chunk.get('id')
            }
        }
        chunks.append(summary_chunk)
        
        return chunks
    
    def generate_summary(self, rubro: str, texto: str) -> str:
        """Genera un resumen ejecutivo de la jurisprudencia."""
        # En producci√≥n, usar Claude para generar resumen
        # Por ahora, extraer primera oraci√≥n significativa
        sentences = texto.split('.')
        key_sentence = next(
            (s for s in sentences if len(s) > 50 and 'debe' in s.lower() or 'procede' in s.lower()),
            sentences[0] if sentences else ''
        )
        return f"RESUMEN: {rubro}\n\nCRITERIO CLAVE: {key_sentence.strip()}."
```

### 3.2.3 Chunking Adaptativo para Documentos de Empresa

```python
class EnterpriseDocumentChunker:
    """
    Chunking para documentos empresariales (contratos, pol√≠ticas, etc.)
    Adapta la estrategia seg√∫n el tipo de documento.
    """
    
    def chunk_contract(self, text: str, metadata: dict) -> list:
        """
        Chunking de contratos por cl√°usulas.
        Mantiene referencias entre cl√°usulas relacionadas.
        """
        chunks = []
        
        # Extraer informaci√≥n de partes
        partes = self.extract_parties(text)
        
        # Extraer cl√°usulas
        clausulas = self.extract_clauses(text)
        
        for i, clausula in enumerate(clausulas):
            chunk = {
                'content': clausula['text'],
                'metadata': {
                    **metadata,
                    'chunk_type': 'contract_clause',
                    'clause_number': clausula['number'],
                    'clause_title': clausula.get('title', ''),
                    'parties': partes,
                    'related_clauses': self.find_related_clauses(clausula, clausulas),
                    'is_fiscal_relevant': self.is_fiscal_relevant(clausula['text']),
                    'risk_level': self.assess_clause_risk(clausula['text']),
                    'keywords': self.extract_contract_keywords(clausula['text'])
                }
            }
            chunks.append(chunk)
        
        # Agregar chunk de resumen del contrato
        summary_chunk = {
            'content': self.generate_contract_summary(text, partes, clausulas),
            'metadata': {
                **metadata,
                'chunk_type': 'contract_summary',
                'total_clauses': len(clausulas),
                'parties': partes,
                'key_terms': self.extract_key_terms(text)
            }
        }
        chunks.append(summary_chunk)
        
        return chunks
    
    def is_fiscal_relevant(self, text: str) -> bool:
        """Determina si una cl√°usula tiene relevancia fiscal."""
        fiscal_keywords = [
            'factura', 'cfdi', 'impuesto', 'iva', 'isr', 'retenci√≥n',
            'deducible', 'precio', 'pago', 'anticipo', 'honorarios',
            'comisi√≥n', 'regal√≠a', 'fiscal', 'tributario'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in fiscal_keywords)
```

## 3.3 Chunking con Contexto Expandido

### 3.3.1 Parent-Child Chunking

```python
class ParentChildChunker:
    """
    Estrategia de chunking que mantiene relaci√≥n padre-hijo.
    √ötil para documentos con estructura jer√°rquica.
    
    - Chunks peque√±os para precisi√≥n en b√∫squeda
    - Chunks padre para contexto amplio en respuesta
    """
    
    def create_hierarchical_chunks(self, document: str, metadata: dict) -> tuple:
        """
        Crea dos niveles de chunks:
        1. Parent chunks: Secciones completas (para contexto)
        2. Child chunks: Subsecciones (para b√∫squeda precisa)
        """
        parent_chunks = []
        child_chunks = []
        
        # Dividir en secciones principales (parents)
        sections = self.split_into_sections(document)
        
        for section in sections:
            parent_id = self.generate_id()
            
            # Crear parent chunk
            parent_chunk = {
                'id': parent_id,
                'content': section['text'],
                'metadata': {
                    **metadata,
                    'chunk_type': 'parent',
                    'section_title': section.get('title', ''),
                    'section_number': section.get('number', ''),
                    'token_count': self.count_tokens(section['text'])
                }
            }
            parent_chunks.append(parent_chunk)
            
            # Crear child chunks
            subsections = self.split_into_subsections(section['text'])
            for i, subsection in enumerate(subsections):
                child_chunk = {
                    'id': self.generate_id(),
                    'content': subsection,
                    'metadata': {
                        **metadata,
                        'chunk_type': 'child',
                        'parent_id': parent_id,
                        'position_in_parent': i,
                        'section_title': section.get('title', ''),
                        'token_count': self.count_tokens(subsection)
                    }
                }
                child_chunks.append(child_chunk)
        
        return parent_chunks, child_chunks
    
    def retrieve_with_context(self, query: str, top_k: int = 5) -> list:
        """
        B√∫squeda que retorna child chunks + sus parents para contexto.
        """
        # Buscar en child chunks (m√°s precisos)
        child_results = self.vector_search(query, collection='children', top_k=top_k)
        
        # Obtener parents para contexto
        parent_ids = set(r['metadata']['parent_id'] for r in child_results)
        parents = self.get_chunks_by_ids(parent_ids, collection='parents')
        
        # Combinar resultados
        results = []
        for child in child_results:
            parent = next(
                (p for p in parents if p['id'] == child['metadata']['parent_id']),
                None
            )
            results.append({
                'match': child,
                'context': parent,
                'relevance_score': child['score']
            })
        
        return results
```

### 3.3.2 Sentence Window Retrieval

```python
class SentenceWindowChunker:
    """
    Estrategia que indexa oraciones individuales pero recupera
    ventanas de contexto alrededor de las coincidencias.
    
    Ideal para: Preguntas espec√≠ficas que necesitan contexto
    """
    
    def __init__(self, window_size: int = 3):
        self.window_size = window_size  # Oraciones antes y despu√©s
    
    def create_sentence_index(self, document: str, metadata: dict) -> list:
        """
        Crea un √≠ndice de oraciones con sus posiciones.
        """
        sentences = self.split_into_sentences(document)
        chunks = []
        
        for i, sentence in enumerate(sentences):
            # Crear ventana de contexto
            start = max(0, i - self.window_size)
            end = min(len(sentences), i + self.window_size + 1)
            window = sentences[start:end]
            
            chunk = {
                'id': self.generate_id(),
                'content': sentence,  # Solo la oraci√≥n para b√∫squeda
                'window_content': ' '.join(window),  # Ventana para respuesta
                'metadata': {
                    **metadata,
                    'chunk_type': 'sentence_window',
                    'sentence_index': i,
                    'window_start': start,
                    'window_end': end,
                    'total_sentences': len(sentences)
                }
            }
            chunks.append(chunk)
        
        return chunks
```

---

# 4. EXTRACCI√ìN Y ENRIQUECIMIENTO DE METADATA

## 4.1 Schema de Metadata por Tipo de Documento

### 4.1.1 Metadata para Documentos Legales

```python
LEGAL_DOCUMENT_SCHEMA = {
    # Identificaci√≥n
    'document_id': str,           # UUID √∫nico
    'document_type': str,         # ley, reglamento, criterio, jurisprudencia
    'source': str,                # DOF, SAT, SCJN, etc.
    
    # Localizaci√≥n legal
    'ordenamiento': str,          # CFF, LISR, LIVA, etc.
    'articulo': str,              # N√∫mero de art√≠culo
    'fraccion': str,              # Fracci√≥n (I, II, III...)
    'inciso': str,                # Inciso (a, b, c...)
    'parrafo': int,               # N√∫mero de p√°rrafo
    
    # Jerarqu√≠a
    'titulo': str,
    'capitulo': str,
    'seccion': str,
    'hierarchy_path': str,        # Ruta completa
    
    # Vigencia
    'fecha_publicacion': date,
    'fecha_inicio_vigencia': date,
    'fecha_fin_vigencia': date,   # null si vigente
    'es_vigente': bool,
    
    # Relaciones
    'articulos_relacionados': list,
    'articulos_derogados': list,
    'reforma_de': str,            # Art√≠culo que reforma
    
    # Sem√°ntico
    'tema_principal': str,
    'temas_secundarios': list,
    'keywords': list,
    'conceptos_fiscales': list,   # T√©rminos del glosario
    
    # T√©cnico
    'token_count': int,
    'embedding_model': str,
    'chunk_strategy': str,
    'created_at': datetime,
    'updated_at': datetime
}
```

### 4.1.2 Metadata para Jurisprudencias

```python
JURISPRUDENCE_SCHEMA = {
    # Identificaci√≥n
    'document_id': str,
    'document_type': 'jurisprudencia' | 'tesis_aislada',
    
    # Localizaci√≥n
    'numero_tesis': str,          # 2a./J. 84/2019
    'registro_digital': str,      # N√∫mero de registro
    'epoca': str,                 # D√©cima √âpoca, etc.
    'instancia': str,             # Segunda Sala, Pleno, TCC
    'fuente': str,                # Semanario Judicial
    'tomo': str,
    'pagina': str,
    
    # Contenido
    'rubro': str,                 # T√≠tulo de la tesis
    'texto': str,
    'precedentes': list,          # Casos que la formaron
    
    # Clasificaci√≥n
    'materia': str,               # Administrativa, Fiscal, Constitucional
    'es_jurisprudencia': bool,    # True = vinculante
    'tipo_jurisprudencia': str,   # Por reiteraci√≥n, contradicci√≥n, etc.
    
    # Relevancia para Durezza
    'articulos_interpretados': list,
    'conceptos_clave': list,
    'aplicabilidad': list,        # deducciones, EFOS, raz√≥n_negocios, etc.
    'score_relevancia': float,    # 0-1 para sistema Durezza
    
    # Relaciones
    'tesis_relacionadas': list,
    'tesis_contradictorias': list,
    'superada_por': str,          # Si ya no es aplicable
}
```

### 4.1.3 Metadata para Documentos de Empresa

```python
ENTERPRISE_DOCUMENT_SCHEMA = {
    # Identificaci√≥n
    'document_id': str,
    'empresa_id': str,
    'document_type': str,         # contrato, acta, estado_financiero, etc.
    
    # Clasificaci√≥n
    'categoria': str,             # constitutivos, fiscales, financieros, etc.
    'subcategoria': str,
    'confidencialidad': str,      # publico, interno, confidencial, restringido
    
    # Temporal
    'fecha_documento': date,
    'periodo_aplica': str,        # Q1 2024, 2024, etc.
    'fecha_vencimiento': date,    # Si aplica
    
    # Partes involucradas
    'partes': list,               # [{nombre, rfc, rol}]
    'firmantes': list,
    'notario': str,               # Si aplica
    
    # Financiero
    'monto': float,
    'moneda': str,
    'tipo_operacion': str,
    
    # Fiscal
    'es_deducible': bool,
    'tipo_deduccion': str,
    'clave_cfdi_relacionada': str,
    'ejercicio_fiscal': int,
    
    # Estado
    'status': str,                # vigente, vencido, cancelado
    'version': int,
    'es_version_final': bool,
    
    # Relaciones
    'proyecto_id': str,           # Si est√° ligado a un proyecto
    'documentos_relacionados': list,
    'cfdi_relacionados': list,
}
```

## 4.2 Extracci√≥n Autom√°tica de Metadata

### 4.2.1 Extractor de Metadata con Claude

```python
class MetadataExtractor:
    """
    Usa Claude para extraer metadata estructurada de documentos.
    """
    
    def __init__(self):
        self.anthropic = Anthropic()
    
    async def extract_legal_metadata(self, text: str, doc_type: str) -> dict:
        """
        Extrae metadata de un documento legal usando Claude.
        """
        
        prompt = f"""Analiza el siguiente fragmento de un documento legal mexicano 
y extrae la metadata estructurada.

TIPO DE DOCUMENTO: {doc_type}

TEXTO:
{text[:4000]}  # Limitar para no exceder contexto

Extrae la siguiente informaci√≥n en formato JSON:
{{
    "ordenamiento": "nombre del c√≥digo o ley",
    "articulo": "n√∫mero de art√≠culo si aplica",
    "fraccion": "fracci√≥n si aplica",
    "tema_principal": "tema principal del fragmento",
    "temas_secundarios": ["lista", "de", "temas"],
    "conceptos_fiscales": ["t√©rminos fiscales clave"],
    "articulos_relacionados": ["art√≠culos mencionados"],
    "es_vigente": true/false,
    "fecha_vigencia": "fecha si se menciona",
    "resumen": "resumen de 1-2 oraciones"
}}

Responde SOLO con el JSON, sin explicaciones adicionales."""

        response = await self.anthropic.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Parsear respuesta JSON
        try:
            metadata = json.loads(response.content[0].text)
            return metadata
        except json.JSONDecodeError:
            # Fallback a extracci√≥n por reglas
            return self.extract_by_rules(text, doc_type)
    
    def extract_by_rules(self, text: str, doc_type: str) -> dict:
        """
        Extracci√≥n basada en reglas como fallback.
        """
        metadata = {}
        
        # Extraer art√≠culos mencionados
        articulos = re.findall(r'art√≠culo\s+(\d+[-\w]*)', text, re.IGNORECASE)
        metadata['articulos_relacionados'] = list(set(articulos))
        
        # Extraer fechas
        fechas = re.findall(r'\d{1,2}\s+de\s+\w+\s+de\s+\d{4}', text)
        if fechas:
            metadata['fechas_mencionadas'] = fechas
        
        # Detectar ordenamientos
        ordenamientos = {
            'CFF': r'C√≥digo Fiscal de la Federaci√≥n|CFF',
            'LISR': r'Ley del Impuesto sobre la Renta|LISR',
            'LIVA': r'Ley del Impuesto al Valor Agregado|LIVA',
        }
        for nombre, pattern in ordenamientos.items():
            if re.search(pattern, text, re.IGNORECASE):
                metadata['ordenamiento'] = nombre
                break
        
        return metadata
```

### 4.2.2 Enriquecimiento con Conceptos del Glosario

```python
class ConceptEnricher:
    """
    Enriquece chunks con conceptos del glosario fiscal.
    Mejora la recuperaci√≥n al agregar sin√≥nimos y t√©rminos relacionados.
    """
    
    def __init__(self, glossary_path: str):
        self.glossary = self.load_glossary(glossary_path)
        self.synonyms = self.load_synonyms()
    
    def enrich_chunk(self, chunk: dict) -> dict:
        """
        Agrega conceptos del glosario encontrados en el chunk.
        """
        text = chunk['content'].lower()
        
        found_concepts = []
        related_terms = []
        
        for concept in self.glossary:
            # Buscar concepto y sus variantes
            variants = [concept['term']] + concept.get('variants', [])
            for variant in variants:
                if variant.lower() in text:
                    found_concepts.append({
                        'term': concept['term'],
                        'definition': concept['definition'],
                        'category': concept.get('category', ''),
                        'related': concept.get('related_terms', [])
                    })
                    related_terms.extend(concept.get('related_terms', []))
                    break
        
        # Actualizar metadata del chunk
        chunk['metadata']['conceptos_fiscales'] = [c['term'] for c in found_concepts]
        chunk['metadata']['terminos_relacionados'] = list(set(related_terms))
        chunk['metadata']['concept_definitions'] = found_concepts
        
        # Agregar sin√≥nimos para mejorar b√∫squeda
        expanded_terms = []
        for concept in found_concepts:
            if concept['term'] in self.synonyms:
                expanded_terms.extend(self.synonyms[concept['term']])
        chunk['metadata']['search_expansion'] = list(set(expanded_terms))
        
        return chunk
```

---

# 5. PIPELINE DE PROCESAMIENTO

## 5.1 Pipeline Completo de Ingesti√≥n

```python
class KnowledgeBaseIngestionPipeline:
    """
    Pipeline completo para ingesti√≥n de documentos al Knowledge Base.
    
    Etapas:
    1. Recepci√≥n y validaci√≥n
    2. Extracci√≥n de texto
    3. Clasificaci√≥n de documento
    4. Chunking seg√∫n tipo
    5. Extracci√≥n de metadata
    6. Enriquecimiento sem√°ntico
    7. Generaci√≥n de embeddings
    8. Indexaci√≥n en vector store
    9. Verificaci√≥n de calidad
    """
    
    def __init__(self, config: dict):
        self.config = config
        self.text_extractor = TextExtractor()
        self.classifier = DocumentClassifier()
        self.chunkers = {
            'legal': LegalDocumentChunker(),
            'jurisprudence': JurisprudenceChunker(),
            'contract': EnterpriseDocumentChunker(),
            'general': GeneralChunker()
        }
        self.metadata_extractor = MetadataExtractor()
        self.enricher = ConceptEnricher(config['glossary_path'])
        self.embedder = EmbeddingGenerator(config['embedding_model'])
        self.vector_store = VectorStore(config['vector_db'])
        self.quality_checker = QualityChecker()
    
    async def ingest_document(
        self, 
        file_path: str, 
        initial_metadata: dict = None
    ) -> IngestionResult:
        """
        Procesa un documento completo a trav√©s del pipeline.
        """
        result = IngestionResult()
        
        try:
            # 1. Validaci√≥n
            self.validate_file(file_path)
            result.log("Archivo validado")
            
            # 2. Extracci√≥n de texto
            text = await self.text_extractor.extract(file_path)
            result.log(f"Texto extra√≠do: {len(text)} caracteres")
            
            # 3. Clasificaci√≥n
            doc_type = await self.classifier.classify(text, file_path)
            result.log(f"Tipo de documento: {doc_type}")
            
            # 4. Chunking
            chunker = self.chunkers.get(doc_type, self.chunkers['general'])
            base_metadata = {
                **(initial_metadata or {}),
                'source_file': file_path,
                'document_type': doc_type,
                'ingestion_date': datetime.now().isoformat()
            }
            chunks = chunker.chunk(text, base_metadata)
            result.log(f"Chunks creados: {len(chunks)}")
            
            # 5. Extracci√≥n de metadata (en paralelo)
            enriched_chunks = await asyncio.gather(*[
                self.extract_and_enrich(chunk, doc_type)
                for chunk in chunks
            ])
            result.log("Metadata extra√≠da y enriquecida")
            
            # 6. Generaci√≥n de embeddings (en batch)
            texts = [c['content'] for c in enriched_chunks]
            embeddings = await self.embedder.embed_batch(texts)
            
            for chunk, embedding in zip(enriched_chunks, embeddings):
                chunk['embedding'] = embedding
            result.log("Embeddings generados")
            
            # 7. Indexaci√≥n
            collection = self.get_collection_for_type(doc_type)
            ids = await self.vector_store.upsert(collection, enriched_chunks)
            result.log(f"Indexados en colecci√≥n: {collection}")
            
            # 8. Verificaci√≥n de calidad
            quality_score = await self.quality_checker.check(enriched_chunks)
            result.quality_score = quality_score
            result.log(f"Score de calidad: {quality_score}")
            
            result.success = True
            result.chunks_created = len(enriched_chunks)
            result.document_id = ids[0] if ids else None
            
        except Exception as e:
            result.success = False
            result.error = str(e)
            result.log(f"Error: {e}")
        
        return result
    
    async def extract_and_enrich(self, chunk: dict, doc_type: str) -> dict:
        """
        Extrae metadata y enriquece un chunk individual.
        """
        # Extraer metadata con Claude
        extracted = await self.metadata_extractor.extract_legal_metadata(
            chunk['content'], doc_type
        )
        chunk['metadata'].update(extracted)
        
        # Enriquecer con conceptos del glosario
        chunk = self.enricher.enrich_chunk(chunk)
        
        return chunk
    
    def get_collection_for_type(self, doc_type: str) -> str:
        """
        Determina la colecci√≥n de vector store seg√∫n el tipo.
        """
        collections = {
            'legal': 'marco_legal',
            'jurisprudence': 'jurisprudencias',
            'contract': 'documentos_empresa',
            'criterion': 'criterios_sat',
            'catalog': 'catalogos',
            'general': 'general'
        }
        return collections.get(doc_type, 'general')
```

## 5.2 Pipeline de Actualizaci√≥n

```python
class KnowledgeBaseUpdater:
    """
    Maneja actualizaciones incrementales al Knowledge Base.
    
    Casos de uso:
    - Nueva versi√≥n de una ley (actualizar art√≠culos modificados)
    - Nueva jurisprudencia (agregar y relacionar)
    - Documento de empresa modificado (versionar)
    - Lista 69-B actualizada (reemplazar)
    """
    
    async def update_legal_document(
        self, 
        new_version_path: str,
        existing_doc_id: str
    ) -> UpdateResult:
        """
        Actualiza un documento legal existente.
        
        Estrategia:
        1. Identificar art√≠culos modificados
        2. Marcar versi√≥n anterior como hist√≥rica
        3. Crear nuevos chunks para art√≠culos modificados
        4. Actualizar relaciones
        """
        result = UpdateResult()
        
        # Obtener documento existente
        existing_chunks = await self.vector_store.get_by_document_id(existing_doc_id)
        
        # Procesar nueva versi√≥n
        new_text = await self.text_extractor.extract(new_version_path)
        new_chunks = self.chunker.chunk(new_text, {'version': 'new'})
        
        # Comparar y detectar cambios
        changes = self.detect_changes(existing_chunks, new_chunks)
        
        for change in changes:
            if change['type'] == 'modified':
                # Marcar chunk anterior como hist√≥rico
                await self.vector_store.update_metadata(
                    change['old_chunk_id'],
                    {'es_vigente': False, 'reemplazado_por': change['new_chunk_id']}
                )
                # Indexar nuevo chunk
                await self.vector_store.upsert('marco_legal', [change['new_chunk']])
                
            elif change['type'] == 'added':
                await self.vector_store.upsert('marco_legal', [change['new_chunk']])
                
            elif change['type'] == 'removed':
                await self.vector_store.update_metadata(
                    change['old_chunk_id'],
                    {'es_vigente': False, 'fecha_derogacion': datetime.now().isoformat()}
                )
        
        result.changes = changes
        return result
    
    async def update_69b_list(self, new_list_path: str) -> UpdateResult:
        """
        Actualiza la lista 69-B del SAT.
        
        Estrategia:
        1. Cargar nueva lista
        2. Comparar con existente
        3. Marcar nuevos presuntos/definitivos
        4. Actualizar metadata de documentos relacionados
        """
        # Cargar nueva lista
        new_list = self.load_69b_list(new_list_path)
        
        # Obtener lista actual
        current_list = await self.vector_store.get_collection('listas_69b')
        
        # Detectar cambios
        new_presuntos = self.find_new_entries(new_list, current_list, 'presunto')
        new_definitivos = self.find_new_entries(new_list, current_list, 'definitivo')
        removed = self.find_removed_entries(new_list, current_list)
        
        # Actualizar
        await self.vector_store.replace_collection('listas_69b', new_list)
        
        # Notificar a proyectos afectados
        await self.notify_affected_projects(new_presuntos + new_definitivos)
        
        return UpdateResult(
            new_presuntos=len(new_presuntos),
            new_definitivos=len(new_definitivos),
            removed=len(removed)
        )
```

---

# 6. SISTEMA DE EMBEDDINGS

## 6.1 Configuraci√≥n de Modelos de Embedding

```python
class EmbeddingGenerator:
    """
    Genera embeddings optimizados para documentos fiscales en espa√±ol.
    """
    
    MODELS = {
        # OpenAI
        'text-embedding-3-large': {
            'dimensions': 3072,
            'max_tokens': 8191,
            'provider': 'openai',
            'best_for': ['general', 'semantic_search']
        },
        'text-embedding-3-small': {
            'dimensions': 1536,
            'max_tokens': 8191,
            'provider': 'openai',
            'best_for': ['cost_effective', 'high_volume']
        },
        
        # Cohere
        'embed-multilingual-v3.0': {
            'dimensions': 1024,
            'max_tokens': 512,
            'provider': 'cohere',
            'best_for': ['spanish', 'multilingual', 'legal']
        },
        
        # Voyage AI (especializado en legal)
        'voyage-law-2': {
            'dimensions': 1024,
            'max_tokens': 16000,
            'provider': 'voyage',
            'best_for': ['legal_documents', 'long_context']
        },
        
        # Local/Self-hosted
        'bge-m3': {
            'dimensions': 1024,
            'max_tokens': 8192,
            'provider': 'local',
            'best_for': ['privacy', 'cost', 'multilingual']
        }
    }
    
    def __init__(self, model_name: str = 'text-embedding-3-large'):
        self.model_name = model_name
        self.model_config = self.MODELS[model_name]
        self.client = self._init_client()
    
    async def embed_batch(
        self, 
        texts: list[str],
        batch_size: int = 100
    ) -> list[list[float]]:
        """
        Genera embeddings en batch para eficiencia.
        """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            # Truncar si excede max_tokens
            batch = [self.truncate(t) for t in batch]
            
            if self.model_config['provider'] == 'openai':
                response = await self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                batch_embeddings = [e.embedding for e in response.data]
            
            elif self.model_config['provider'] == 'cohere':
                response = await self.client.embed(
                    texts=batch,
                    model=self.model_name,
                    input_type='search_document'
                )
                batch_embeddings = response.embeddings
            
            embeddings.extend(batch_embeddings)
        
        return embeddings
    
    async def embed_query(self, query: str) -> list[float]:
        """
        Genera embedding para una query de b√∫squeda.
        Usa input_type diferente para mejor retrieval.
        """
        if self.model_config['provider'] == 'cohere':
            response = await self.client.embed(
                texts=[query],
                model=self.model_name,
                input_type='search_query'  # Diferente para queries
            )
            return response.embeddings[0]
        else:
            response = await self.client.embeddings.create(
                model=self.model_name,
                input=[query]
            )
            return response.data[0].embedding
```

## 6.2 Estrategia de Embedding H√≠brido

```python
class HybridEmbeddingStrategy:
    """
    Combina embeddings densos con b√∫squeda por palabras clave (sparse).
    Mejora significativamente la recuperaci√≥n de t√©rminos t√©cnicos.
    """
    
    def __init__(self):
        self.dense_embedder = EmbeddingGenerator('text-embedding-3-large')
        self.sparse_embedder = BM25Encoder()  # Para t√©rminos exactos
    
    async def create_hybrid_embedding(self, text: str) -> dict:
        """
        Crea representaci√≥n h√≠brida del texto.
        """
        # Embedding denso para sem√°ntica
        dense = await self.dense_embedder.embed_batch([text])
        
        # Embedding sparse para t√©rminos exactos
        # Especialmente importante para: RFC, n√∫meros de art√≠culo, claves SAT
        sparse = self.sparse_embedder.encode(text)
        
        # Extraer t√©rminos t√©cnicos para boosting
        technical_terms = self.extract_technical_terms(text)
        
        return {
            'dense_embedding': dense[0],
            'sparse_embedding': sparse,
            'technical_terms': technical_terms,
            'term_weights': self.calculate_term_weights(technical_terms)
        }
    
    def extract_technical_terms(self, text: str) -> list:
        """
        Extrae t√©rminos t√©cnicos fiscales que deben tener match exacto.
        """
        patterns = [
            r'[A-Z]{3,4}\d{6}[A-Z\d]{3}',  # RFC
            r'art√≠culo\s+\d+[-\w]*',         # Art√≠culos
            r'\d{8}',                         # Claves SAT (8 d√≠gitos)
            r'[IVXLCDM]+',                   # Fracciones romanas
            r'DOF\s+\d{2}/\d{2}/\d{4}',     # Referencias DOF
        ]
        
        terms = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            terms.extend(matches)
        
        return list(set(terms))
    
    async def hybrid_search(
        self, 
        query: str, 
        collection: str,
        top_k: int = 10,
        alpha: float = 0.7  # Peso de dense vs sparse
    ) -> list:
        """
        B√∫squeda h√≠brida combinando dense y sparse.
        """
        # Embedding de la query
        query_dense = await self.dense_embedder.embed_query(query)
        query_sparse = self.sparse_embedder.encode(query)
        query_terms = self.extract_technical_terms(query)
        
        # B√∫squeda dense
        dense_results = await self.vector_store.search(
            collection=collection,
            vector=query_dense,
            top_k=top_k * 2  # M√°s resultados para fusionar
        )
        
        # B√∫squeda sparse (BM25)
        sparse_results = await self.bm25_index.search(
            collection=collection,
            query=query,
            top_k=top_k * 2
        )
        
        # Boost para t√©rminos t√©cnicos exactos
        boosted_results = self.boost_exact_matches(
            dense_results + sparse_results,
            query_terms
        )
        
        # Fusionar con Reciprocal Rank Fusion
        fused = self.reciprocal_rank_fusion(
            dense_results,
            sparse_results,
            boosted_results,
            weights=[alpha, 1-alpha, 0.3]
        )
        
        return fused[:top_k]
```

---

# 7. VECTOR STORE Y INDEXACI√ìN

## 7.1 Configuraci√≥n de Vector Store

```python
class VectorStoreManager:
    """
    Gestiona la conexi√≥n y operaciones con el vector store.
    Soporta: Pinecone, Weaviate, Qdrant, Chroma, pgvector
    """
    
    def __init__(self, config: dict):
        self.provider = config['provider']
        self.client = self._init_client(config)
        self.collections = {}
    
    async def create_collection(
        self, 
        name: str, 
        schema: dict,
        index_config: dict = None
    ):
        """
        Crea una colecci√≥n con schema definido.
        """
        if self.provider == 'pinecone':
            self.client.create_index(
                name=name,
                dimension=schema['embedding_dimension'],
                metric='cosine',
                spec=ServerlessSpec(
                    cloud='aws',
                    region='us-east-1'
                )
            )
        
        elif self.provider == 'qdrant':
            await self.client.create_collection(
                collection_name=name,
                vectors_config=VectorParams(
                    size=schema['embedding_dimension'],
                    distance=Distance.COSINE
                ),
                # √çndices para filtrado eficiente
                payload_schema={
                    'document_type': PayloadSchemaType.KEYWORD,
                    'ordenamiento': PayloadSchemaType.KEYWORD,
                    'articulo': PayloadSchemaType.KEYWORD,
                    'es_vigente': PayloadSchemaType.BOOL,
                    'fecha_publicacion': PayloadSchemaType.DATETIME,
                    'empresa_id': PayloadSchemaType.KEYWORD,
                }
            )
        
        elif self.provider == 'weaviate':
            class_obj = {
                'class': name,
                'vectorizer': 'none',  # Usamos nuestros embeddings
                'properties': self._schema_to_weaviate(schema),
                'vectorIndexConfig': {
                    'distance': 'cosine',
                    'ef': 100,
                    'efConstruction': 128,
                    'maxConnections': 64
                }
            }
            self.client.schema.create_class(class_obj)
    
    async def upsert(
        self, 
        collection: str, 
        chunks: list[dict]
    ) -> list[str]:
        """
        Inserta o actualiza chunks en la colecci√≥n.
        """
        ids = []
        
        if self.provider == 'qdrant':
            points = [
                PointStruct(
                    id=chunk.get('id', str(uuid4())),
                    vector=chunk['embedding'],
                    payload={
                        'content': chunk['content'],
                        **chunk['metadata']
                    }
                )
                for chunk in chunks
            ]
            
            await self.client.upsert(
                collection_name=collection,
                points=points
            )
            ids = [p.id for p in points]
        
        return ids
    
    async def search(
        self, 
        collection: str,
        vector: list[float],
        filter: dict = None,
        top_k: int = 10
    ) -> list[dict]:
        """
        B√∫squeda por similitud con filtros opcionales.
        """
        if self.provider == 'qdrant':
            # Construir filtro Qdrant
            qdrant_filter = self._build_qdrant_filter(filter) if filter else None
            
            results = await self.client.search(
                collection_name=collection,
                query_vector=vector,
                query_filter=qdrant_filter,
                limit=top_k,
                with_payload=True
            )
            
            return [
                {
                    'id': r.id,
                    'score': r.score,
                    'content': r.payload.get('content'),
                    'metadata': {k: v for k, v in r.payload.items() if k != 'content'}
                }
                for r in results
            ]
    
    def _build_qdrant_filter(self, filter: dict) -> Filter:
        """
        Construye filtro Qdrant desde diccionario.
        
        Ejemplo filter:
        {
            'ordenamiento': 'CFF',
            'es_vigente': True,
            'fecha_publicacion': {'$gte': '2024-01-01'}
        }
        """
        conditions = []
        
        for key, value in filter.items():
            if isinstance(value, dict):
                # Operadores: $gte, $lte, $in, etc.
                for op, val in value.items():
                    if op == '$gte':
                        conditions.append(
                            FieldCondition(key=key, range=Range(gte=val))
                        )
                    elif op == '$lte':
                        conditions.append(
                            FieldCondition(key=key, range=Range(lte=val))
                        )
                    elif op == '$in':
                        conditions.append(
                            FieldCondition(key=key, match=MatchAny(any=val))
                        )
            else:
                # Match exacto
                conditions.append(
                    FieldCondition(key=key, match=MatchValue(value=value))
                )
        
        return Filter(must=conditions)
```

## 7.2 √çndices Optimizados para Consultas Frecuentes

```python
class IndexOptimizer:
    """
    Crea y mantiene √≠ndices optimizados para las consultas
    m√°s frecuentes del sistema Durezza.
    """
    
    # Consultas frecuentes y sus √≠ndices √≥ptimos
    QUERY_PATTERNS = {
        'buscar_articulo': {
            'description': 'Buscar art√≠culo espec√≠fico de una ley',
            'example': 'Art√≠culo 27 fracci√≥n I de la LISR',
            'filters': ['ordenamiento', 'articulo', 'fraccion'],
            'index_type': 'keyword'
        },
        'jurisprudencia_tema': {
            'description': 'Buscar jurisprudencias sobre un tema',
            'example': 'Jurisprudencias sobre raz√≥n de negocios',
            'filters': ['document_type', 'materia', 'es_jurisprudencia'],
            'index_type': 'hybrid'
        },
        'documentos_empresa': {
            'description': 'Buscar documentos de una empresa',
            'example': 'Contratos de Empresa X en 2024',
            'filters': ['empresa_id', 'document_type', 'fecha_documento'],
            'index_type': 'filtered'
        },
        'vigencia_legal': {
            'description': 'Buscar disposiciones vigentes',
            'example': 'Requisitos vigentes para deducciones',
            'filters': ['es_vigente', 'ordenamiento'],
            'index_type': 'filtered_semantic'
        },
        'proveedor_69b': {
            'description': 'Verificar si RFC est√° en lista 69-B',
            'example': 'Verificar RFC ABC123456XYZ',
            'filters': ['rfc'],
            'index_type': 'exact_match'
        }
    }
    
    async def create_optimized_indexes(self, vector_store: VectorStoreManager):
        """
        Crea √≠ndices optimizados para cada patr√≥n de consulta.
        """
        for pattern_name, config in self.QUERY_PATTERNS.items():
            for field in config['filters']:
                await vector_store.create_index(
                    collection=self.get_collection_for_pattern(pattern_name),
                    field=field,
                    index_type=config['index_type']
                )
```

---

# 8. VERIFICACI√ìN DE CALIDAD

## 8.1 Quality Checker

```python
class QualityChecker:
    """
    Verifica la calidad de los chunks antes de indexar.
    Detecta problemas comunes y sugiere mejoras.
    """
    
    QUALITY_THRESHOLDS = {
        'min_chunk_length': 100,        # Caracteres m√≠nimos
        'max_chunk_length': 3000,       # Caracteres m√°ximos
        'min_metadata_fields': 5,       # Campos de metadata m√≠nimos
        'required_metadata': [
            'document_type', 
            'source_file', 
            'ingestion_date'
        ],
        'min_keyword_count': 3,         # Keywords m√≠nimos
        'max_overlap_ratio': 0.5,       # M√°ximo overlap entre chunks
    }
    
    async def check(self, chunks: list[dict]) -> QualityReport:
        """
        Ejecuta todas las verificaciones de calidad.
        """
        report = QualityReport()
        
        for i, chunk in enumerate(chunks):
            chunk_issues = []
            
            # Verificar longitud
            if len(chunk['content']) < self.QUALITY_THRESHOLDS['min_chunk_length']:
                chunk_issues.append({
                    'type': 'too_short',
                    'message': f"Chunk muy corto: {len(chunk['content'])} caracteres",
                    'severity': 'warning'
                })
            
            if len(chunk['content']) > self.QUALITY_THRESHOLDS['max_chunk_length']:
                chunk_issues.append({
                    'type': 'too_long',
                    'message': f"Chunk muy largo: {len(chunk['content'])} caracteres",
                    'severity': 'warning'
                })
            
            # Verificar metadata requerida
            for field in self.QUALITY_THRESHOLDS['required_metadata']:
                if field not in chunk.get('metadata', {}):
                    chunk_issues.append({
                        'type': 'missing_metadata',
                        'message': f"Campo requerido faltante: {field}",
                        'severity': 'error'
                    })
            
            # Verificar contenido significativo
            if self.is_boilerplate(chunk['content']):
                chunk_issues.append({
                    'type': 'boilerplate',
                    'message': "Contenido parece ser boilerplate/repetitivo",
                    'severity': 'warning'
                })
            
            # Verificar que no est√© truncado
            if self.appears_truncated(chunk['content']):
                chunk_issues.append({
                    'type': 'truncated',
                    'message': "Chunk parece estar truncado",
                    'severity': 'warning'
                })
            
            # Verificar coherencia sem√°ntica
            coherence_score = await self.check_semantic_coherence(chunk['content'])
            if coherence_score < 0.5:
                chunk_issues.append({
                    'type': 'low_coherence',
                    'message': f"Baja coherencia sem√°ntica: {coherence_score:.2f}",
                    'severity': 'warning'
                })
            
            if chunk_issues:
                report.add_issues(i, chunk_issues)
        
        # Verificar overlap entre chunks
        overlap_issues = self.check_chunk_overlap(chunks)
        report.add_global_issues(overlap_issues)
        
        # Calcular score general
        report.calculate_score()
        
        return report
    
    def is_boilerplate(self, text: str) -> bool:
        """Detecta si el texto es contenido repetitivo/gen√©rico."""
        boilerplate_patterns = [
            r'^(p√°gina|page)\s+\d+',
            r'(todos los derechos reservados|all rights reserved)',
            r'^√≠ndice$',
            r'^tabla de contenido',
        ]
        text_lower = text.lower().strip()
        return any(re.match(p, text_lower) for p in boilerplate_patterns)
    
    def appears_truncated(self, text: str) -> bool:
        """Detecta si el texto parece estar cortado abruptamente."""
        # Termina en medio de una palabra
        if re.search(r'\w$', text) and not text.endswith('.'):
            return True
        # Termina con conectores
        if re.search(r'\b(y|o|que|de|del|la|el)\s*$', text.lower()):
            return True
        return False
    
    async def check_semantic_coherence(self, text: str) -> float:
        """
        Usa Claude para evaluar coherencia sem√°ntica del chunk.
        """
        prompt = f"""Eval√∫a si el siguiente texto es un fragmento coherente 
y autocontenido que podr√≠a ser √∫til para responder preguntas.

TEXTO:
{text[:1000]}

Responde con un n√∫mero entre 0 y 1:
- 1.0 = Completamente coherente, autocontenido, √∫til
- 0.5 = Parcialmente coherente, necesita algo de contexto
- 0.0 = Incoherente, fragmentado, no √∫til

Responde SOLO con el n√∫mero."""

        response = await self.anthropic.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=10,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            return float(response.content[0].text.strip())
        except:
            return 0.5  # Default si no puede parsear
```

## 8.2 Deduplicaci√≥n

```python
class Deduplicator:
    """
    Detecta y maneja contenido duplicado o muy similar.
    """
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.threshold = similarity_threshold
    
    async def find_duplicates(
        self, 
        new_chunks: list[dict],
        existing_collection: str
    ) -> list[DuplicateResult]:
        """
        Encuentra chunks duplicados o muy similares.
        """
        duplicates = []
        
        for chunk in new_chunks:
            # Buscar similares en colecci√≥n existente
            similar = await self.vector_store.search(
                collection=existing_collection,
                vector=chunk['embedding'],
                top_k=5
            )
            
            for match in similar:
                if match['score'] >= self.threshold:
                    duplicates.append(DuplicateResult(
                        new_chunk=chunk,
                        existing_chunk=match,
                        similarity=match['score'],
                        action=self.determine_action(chunk, match)
                    ))
        
        return duplicates
    
    def determine_action(self, new: dict, existing: dict) -> str:
        """
        Determina qu√© hacer con un duplicado.
        """
        # Si el nuevo es m√°s reciente, actualizar
        new_date = new['metadata'].get('fecha_publicacion', '')
        existing_date = existing['metadata'].get('fecha_publicacion', '')
        
        if new_date > existing_date:
            return 'update'
        elif new_date == existing_date:
            return 'skip'  # Ya existe, no hacer nada
        else:
            return 'keep_existing'  # El existente es m√°s nuevo
```

---

# 9. API DEL KNOWLEDGE BASE

## 9.1 Endpoints de Ingesti√≥n

```typescript
// server/routes/knowledgeBase.ts

import { Router } from 'express';
import { KnowledgeBaseIngestionPipeline } from '../services/kb/pipeline';
import { KnowledgeBaseUpdater } from '../services/kb/updater';
import multer from 'multer';

const router = Router();
const upload = multer({ dest: 'uploads/' });
const pipeline = new KnowledgeBaseIngestionPipeline(config);
const updater = new KnowledgeBaseUpdater(config);

// POST /api/kb/ingest
// Ingestar un nuevo documento
router.post('/ingest', upload.single('file'), async (req, res) => {
  try {
    const { file } = req;
    const { documentType, metadata } = req.body;
    
    const result = await pipeline.ingestDocument(
      file.path,
      {
        document_type: documentType,
        ...JSON.parse(metadata || '{}')
      }
    );
    
    if (result.success) {
      return res.json({
        success: true,
        documentId: result.document_id,
        chunksCreated: result.chunks_created,
        qualityScore: result.quality_score
      });
    } else {
      return res.status(400).json({
        success: false,
        error: result.error
      });
    }
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/ingest/batch
// Ingestar m√∫ltiples documentos
router.post('/ingest/batch', upload.array('files', 50), async (req, res) => {
  try {
    const files = req.files as Express.Multer.File[];
    const { documentType, metadata } = req.body;
    
    const results = await Promise.all(
      files.map(file => 
        pipeline.ingestDocument(file.path, {
          document_type: documentType,
          ...JSON.parse(metadata || '{}')
        })
      )
    );
    
    return res.json({
      total: files.length,
      successful: results.filter(r => r.success).length,
      failed: results.filter(r => !r.success).length,
      results
    });
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/ingest/url
// Ingestar desde URL (para DOF, SAT, etc.)
router.post('/ingest/url', async (req, res) => {
  try {
    const { url, documentType, metadata } = req.body;
    
    // Descargar documento
    const filePath = await downloadDocument(url);
    
    const result = await pipeline.ingestDocument(filePath, {
      document_type: documentType,
      source_url: url,
      ...metadata
    });
    
    return res.json(result);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// PUT /api/kb/update/legal/:documentId
// Actualizar documento legal existente
router.put('/update/legal/:documentId', upload.single('file'), async (req, res) => {
  try {
    const { documentId } = req.params;
    const { file } = req;
    
    const result = await updater.updateLegalDocument(
      file.path,
      documentId
    );
    
    return res.json(result);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/update/69b
// Actualizar lista 69-B
router.post('/update/69b', upload.single('file'), async (req, res) => {
  try {
    const result = await updater.update69bList(req.file.path);
    
    return res.json({
      success: true,
      newPresuntos: result.new_presuntos,
      newDefinitivos: result.new_definitivos,
      removed: result.removed
    });
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

export default router;
```

## 9.2 Endpoints de B√∫squeda

```typescript
// server/routes/kbSearch.ts

import { Router } from 'express';
import { KnowledgeBaseSearcher } from '../services/kb/searcher';

const router = Router();
const searcher = new KnowledgeBaseSearcher(config);

// POST /api/kb/search
// B√∫squeda sem√°ntica general
router.post('/search', async (req, res) => {
  try {
    const { 
      query, 
      collections,  // ['marco_legal', 'jurisprudencias']
      filters,      // { ordenamiento: 'CFF', es_vigente: true }
      topK = 10,
      includeContext = true
    } = req.body;
    
    const results = await searcher.search({
      query,
      collections,
      filters,
      topK,
      includeContext
    });
    
    return res.json(results);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/search/article
// B√∫squeda de art√≠culo espec√≠fico
router.post('/search/article', async (req, res) => {
  try {
    const { ordenamiento, articulo, fraccion } = req.body;
    
    const result = await searcher.findArticle({
      ordenamiento,
      articulo,
      fraccion
    });
    
    return res.json(result);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/search/jurisprudencia
// B√∫squeda de jurisprudencias
router.post('/search/jurisprudencia', async (req, res) => {
  try {
    const { 
      query, 
      materia,
      soloJurisprudencias = false,
      articulosRelacionados
    } = req.body;
    
    const results = await searcher.findJurisprudencias({
      query,
      filters: {
        materia,
        es_jurisprudencia: soloJurisprudencias || undefined
      },
      articulosRelacionados
    });
    
    return res.json(results);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// GET /api/kb/verify/69b/:rfc
// Verificar RFC en lista 69-B
router.get('/verify/69b/:rfc', async (req, res) => {
  try {
    const { rfc } = req.params;
    
    const result = await searcher.verify69B(rfc);
    
    return res.json({
      rfc,
      encontrado: result.found,
      tipo: result.type,  // 'presunto' | 'definitivo' | null
      fechaPublicacion: result.date,
      detalles: result.details
    });
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

// POST /api/kb/context
// Obtener contexto para un agente
router.post('/context', async (req, res) => {
  try {
    const { 
      agenteId,     // 'A1', 'A3', etc.
      proyectoId,
      query,
      maxTokens = 4000
    } = req.body;
    
    // Obtener contexto relevante seg√∫n el agente
    const context = await searcher.getAgentContext({
      agenteId,
      proyectoId,
      query,
      maxTokens
    });
    
    return res.json(context);
  } catch (error) {
    return res.status(500).json({ error: error.message });
  }
});

export default router;
```

---

# 10. SYSTEM PROMPT DEL AGENTE KB

## 10.1 Prompt Completo

```markdown
# IDENTIDAD

Eres la Dra. Elena V√°zquez, Directora de Gesti√≥n del Conocimiento de Durezza.
Tu especialidad es la organizaci√≥n, clasificaci√≥n e indexaci√≥n de informaci√≥n
fiscal mexicana para sistemas de recuperaci√≥n aumentada (RAG).

# MISI√ìN

Tu misi√≥n es asegurar que toda la informaci√≥n que ingresa al Knowledge Base
est√© perfectamente organizada, enriquecida y lista para ser encontrada
por los otros agentes cuando la necesiten.

# PRINCIPIOS FUNDAMENTALES

1. **Calidad sobre cantidad**: Un chunk mal indexado es peor que no tenerlo
2. **Contexto es clave**: Cada fragmento debe poder entenderse solo
3. **Metadata precisa**: La metadata incorrecta = informaci√≥n invisible
4. **Jerarqu√≠a respetada**: Mantener la estructura original del documento
5. **Relaciones expl√≠citas**: Conectar informaci√≥n relacionada

# CONOCIMIENTO ESPECIALIZADO

## Tipos de Documentos que Procesas

1. **Leyes y C√≥digos**
   - Estrategia: Por art√≠culo con contexto de jerarqu√≠a
   - Metadata clave: ordenamiento, art√≠culo, fracci√≥n, vigencia
   
2. **Jurisprudencias y Tesis**
   - Estrategia: Completa + resumen ejecutivo
   - Metadata clave: √©poca, instancia, materia, rubro, es_jurisprudencia
   
3. **Criterios SAT**
   - Estrategia: Por criterio individual
   - Metadata clave: n√∫mero, fecha, vinculante, materia
   
4. **Contratos y Documentos Empresariales**
   - Estrategia: Por cl√°usula con referencias
   - Metadata clave: partes, fecha, tipo, monto, es_deducible
   
5. **Cat√°logos SAT**
   - Estrategia: Por registro individual
   - Metadata clave: clave, descripci√≥n, vigencia

## Taxonom√≠a Fiscal Mexicana

Organizas el contenido bajo estas categor√≠as principales:

- **Deducciones**: Todo lo relacionado con Art. 25, 27, 28 LISR
- **Comprobantes**: CFDI, requisitos Art. 29, 29-A CFF
- **EFOS**: Art. 69-B, listas, criterios de detecci√≥n
- **Raz√≥n de Negocios**: Art. 5-A CFF, sustancia econ√≥mica
- **Precios de Transferencia**: Art. 179, 180 LISR
- **Retenciones**: ISR, IVA de servicios
- **Cumplimiento**: Obligaciones, plazos, declaraciones

# TAREAS QUE REALIZAS

## 1. Clasificaci√≥n de Documentos

Cuando recibes un documento:
1. Identifico el tipo (ley, jurisprudencia, contrato, etc.)
2. Extraigo metadata inicial del nombre y contenido
3. Determino la colecci√≥n destino
4. Selecciono la estrategia de chunking apropiada

## 2. Chunking Inteligente

Para cada documento:
1. Analizo la estructura (t√≠tulos, secciones, art√≠culos)
2. Identifico puntos de corte sem√°nticos
3. Creo chunks de tama√±o √≥ptimo para RAG (400-800 tokens)
4. Agrego overlap para mantener contexto
5. Preservo referencias entre chunks relacionados

## 3. Extracci√≥n de Metadata

Para cada chunk extraigo:
- Identificadores (art√≠culo, n√∫mero de tesis, etc.)
- Fechas relevantes (publicaci√≥n, vigencia)
- Conceptos fiscales clave
- Referencias a otros documentos
- T√©rminos t√©cnicos para b√∫squeda

## 4. Enriquecimiento Sem√°ntico

Mejoro cada chunk con:
- Sin√≥nimos de t√©rminos t√©cnicos
- Definiciones de conceptos
- Relaciones con otros chunks
- Tags de categorizaci√≥n
- Score de relevancia para diferentes agentes

## 5. Verificaci√≥n de Calidad

Antes de indexar verifico:
- Longitud adecuada del chunk
- Metadata completa y correcta
- Coherencia sem√°ntica del contenido
- Ausencia de duplicados
- Referencias v√°lidas

# FORMATO DE RESPUESTAS

Cuando proceso un documento, reporto:

```
## RESULTADO DE PROCESAMIENTO

**Documento:** [Nombre del archivo]
**Tipo detectado:** [ley/jurisprudencia/contrato/etc.]
**Colecci√≥n destino:** [marco_legal/jurisprudencias/etc.]

### Estad√≠sticas
- P√°ginas procesadas: X
- Chunks creados: Y
- Tama√±o promedio: Z tokens
- Score de calidad: W/100

### Metadata Extra√≠da
- Ordenamiento: [si aplica]
- Art√≠culos: [lista]
- Conceptos clave: [lista]
- Fechas relevantes: [lista]

### Observaciones
- [Cualquier issue o recomendaci√≥n]

### Siguientes pasos
- [Si requiere acci√≥n del usuario]
```

# INSTRUCCIONES ESPEC√çFICAS

## Para Leyes Mexicanas

1. Respetar estructura: T√≠tulo > Cap√≠tulo > Secci√≥n > Art√≠culo > Fracci√≥n
2. Incluir el n√∫mero de art√≠culo en CADA chunk
3. Marcar si est√° vigente o derogado
4. Extraer referencias a otros art√≠culos
5. Identificar fechas de publicaci√≥n en DOF

## Para Jurisprudencias

1. Extraer el rubro completo
2. Identificar si es jurisprudencia (J) o tesis aislada (TA)
3. Determinar la √©poca y la instancia
4. Clasificar por materia
5. Vincular con art√≠culos que interpreta

## Para Documentos de Empresa

1. Asociar siempre con empresa_id
2. Clasificar nivel de confidencialidad
3. Identificar partes involucradas
4. Extraer montos y fechas clave
5. Marcar relevancia fiscal

## Para Cat√°logos SAT

1. Mantener estructura de clave-valor
2. Preservar jerarqu√≠a (divisi√≥n > grupo > clase)
3. Incluir descripciones completas
4. Marcar vigencia

# HERRAMIENTAS DISPONIBLES

1. **process_document**: Procesa un documento completo
2. **extract_metadata**: Extrae metadata de un fragmento
3. **classify_document**: Clasifica tipo de documento
4. **chunk_document**: Divide documento en chunks
5. **enrich_chunk**: Enriquece un chunk con conceptos
6. **validate_chunk**: Verifica calidad de un chunk
7. **search_similar**: Busca chunks similares (para deduplicaci√≥n)
8. **update_collection**: Actualiza chunks existentes

# RESPUESTA A CONSULTAS

Cuando un agente o usuario pregunta sobre el Knowledge Base:

1. Explico claramente qu√© contiene cada colecci√≥n
2. Indico c√≥mo est√° organizada la informaci√≥n
3. Sugiero mejores formas de buscar si la query es vaga
4. Reporto si hay informaci√≥n faltante o desactualizada
5. Propongo documentos a agregar si identifico gaps

# MANTENIMIENTO

Peri√≥dicamente:
1. Identifico informaci√≥n desactualizada
2. Sugiero actualizaciones necesarias
3. Reporto estad√≠sticas de uso
4. Recomiendo optimizaciones de indexaci√≥n
5. Detecto y fusiono duplicados
```

---

# 11. GLOSARIO Y CONCEPTOS

## 11.1 Estructura del Glosario

```json
{
  "glossary": [
    {
      "term": "Raz√≥n de negocios",
      "variants": ["razon de negocios", "business purpose", "motivo de negocio"],
      "definition": "Justificaci√≥n econ√≥mica y empresarial de una operaci√≥n, m√°s all√° del beneficio fiscal que pueda generar. Requisito establecido en el Art. 5-A del CFF.",
      "category": "conceptos_fiscales",
      "related_terms": ["sustancia econ√≥mica", "beneficio fiscal", "simulaci√≥n"],
      "legal_references": ["Art. 5-A CFF"],
      "agents": ["A1", "A3"],
      "synonyms_for_search": ["prop√≥sito comercial", "motivaci√≥n empresarial", "justificaci√≥n de negocio"]
    },
    {
      "term": "EFOS",
      "variants": ["efos", "empresas facturadoras de operaciones simuladas"],
      "definition": "Empresas que emiten comprobantes fiscales sin tener capacidad material, directa o indirectamente, para prestar los servicios o entregar los bienes que amparan dichos comprobantes.",
      "category": "efos",
      "related_terms": ["EDOS", "operaci√≥n simulada", "lista 69-B", "presuntos", "definitivos"],
      "legal_references": ["Art. 69-B CFF"],
      "agents": ["A6", "A3"],
      "synonyms_for_search": ["factureras", "empresas fantasma", "empresas de papel", "facturaci√≥n ap√≥crifa"]
    },
    {
      "term": "Materialidad",
      "variants": ["materialidad de la operaci√≥n", "prueba de materialidad"],
      "definition": "Evidencia de que una operaci√≥n realmente se llev√≥ a cabo, incluyendo la prestaci√≥n efectiva del servicio o entrega del bien, capacidad del proveedor, y existencia de entregables tangibles.",
      "category": "deducciones",
      "related_terms": ["sustancia", "evidencia", "comprobaci√≥n", "capacidad operativa"],
      "legal_references": ["Art. 27 LISR", "Art. 69-B CFF"],
      "agents": ["A3", "A6"],
      "synonyms_for_search": ["prueba del servicio", "evidencia de prestaci√≥n", "comprobaci√≥n de operaci√≥n"]
    }
  ]
}
```

---

# 12. M√âTRICAS Y MONITOREO

## 12.1 KPIs del Knowledge Base

| M√©trica | Descripci√≥n | Meta |
|---------|-------------|------|
| **Cobertura** | % de temas fiscales cubiertos | >95% |
| **Frescura** | Edad promedio de documentos legales | <6 meses |
| **Precisi√≥n de b√∫squeda** | % de queries con resultado relevante en top 5 | >90% |
| **Calidad de chunks** | Score promedio de quality checker | >80/100 |
| **Duplicados** | % de chunks duplicados | <2% |
| **Tiempo de ingesti√≥n** | Segundos por documento | <30s |
| **Uso por agente** | Queries por agente por d√≠a | Monitorear |

---

*SKILL del Agente KB - Knowledge Base Manager*
*Plataforma: Durezza 4.0*
*Optimizado para RAG con documentos fiscales mexicanos*